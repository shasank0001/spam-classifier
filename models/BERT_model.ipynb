{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/shasank/anaconda3/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Imports\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import tensorflow as tf\n",
    "import keras_nlp\n",
    "from tensorflow.keras.layers import Dense, Dropout, Input\n",
    "from tensorflow.keras.models import Model\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Load and Preprocess Data\n",
    "# Load your email dataset\n",
    "df = pd.read_csv('/home/shasank/shasank/ml/projects/email-classifier/dataSets/toy_dataSet.csv')\n",
    "\n",
    "df.head()\n",
    "df.rename(columns={'Category': 'label','Message': 'email_text'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Encode labels\n",
    "label_encoder = LabelEncoder()\n",
    "encoded_labels = label_encoder.fit_transform(df['label'])\n",
    "num_classes = len(label_encoder.classes_)\n",
    "\n",
    "# Split data\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    df['email_text'].values, \n",
    "    encoded_labels,\n",
    "    test_size=0.2, \n",
    "    random_state=42\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Available BERT presets:\n",
      "dict_keys(['bert_tiny_en_uncased', 'bert_small_en_uncased', 'bert_medium_en_uncased', 'bert_base_en_uncased', 'bert_base_en', 'bert_base_zh', 'bert_base_multi', 'bert_large_en_uncased', 'bert_large_en', 'bert_tiny_en_uncased_sst2'])\n",
      "Training data shape: (4457, 128)\n",
      "Test data shape: (1115, 128)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-12 18:45:17.216690: E tensorflow/core/util/util.cc:131] oneDNN supports DT_INT64 only on platforms with AVX-512. Falling back to the default Eigen-based implementation if present.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: Initialize BERT Classifier and Preprocessor\n",
    "# Display available presets\n",
    "print(\"Available BERT presets:\")\n",
    "print(keras_nlp.models.BertClassifier.presets.keys())\n",
    "\n",
    "# Initialize the classifier\n",
    "classifier = keras_nlp.models.BertClassifier.from_preset(\n",
    "    \"bert_base_en\",\n",
    "    num_classes=num_classes\n",
    ")\n",
    "\n",
    "# Configure sequence length in preprocessor\n",
    "classifier.preprocessor.sequence_length = 128\n",
    "\n",
    "# Preprocess the data\n",
    "x_train = classifier.preprocessor(X_train)\n",
    "x_test = classifier.preprocessor(X_test)\n",
    "\n",
    "# Print shape to verify\n",
    "print(f\"Training data shape: {x_train['token_ids'].shape}\")\n",
    "print(f\"Test data shape: {x_test['token_ids'].shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "(slice(None, None, None), 0, slice(None, None, None))",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 12\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# Get BERT outputs\u001b[39;00m\n\u001b[1;32m     11\u001b[0m sequence_output \u001b[38;5;241m=\u001b[39m bert_backbone([input_ids, input_mask, input_type_ids])\n\u001b[0;32m---> 12\u001b[0m pooled_output \u001b[38;5;241m=\u001b[39m \u001b[43msequence_output\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m  \u001b[38;5;66;03m# Use [CLS] token output\u001b[39;00m\n\u001b[1;32m     14\u001b[0m \u001b[38;5;66;03m# Add classification head\u001b[39;00m\n\u001b[1;32m     15\u001b[0m x \u001b[38;5;241m=\u001b[39m Dropout(\u001b[38;5;241m0.1\u001b[39m)(pooled_output)\n",
      "\u001b[0;31mKeyError\u001b[0m: (slice(None, None, None), 0, slice(None, None, None))"
     ]
    }
   ],
   "source": [
    "# Cell 4: Build the Model\n",
    "# Initialize BERT backbone\n",
    "bert_backbone = keras_nlp.models.BertBackbone.from_preset(\"bert_base_en\")\n",
    "\n",
    "# Define input layers\n",
    "input_ids = Input(shape=(128,), dtype=tf.int32, name=\"token_ids\")\n",
    "input_mask = Input(shape=(128,), dtype=tf.int32, name=\"padding_mask\")\n",
    "input_type_ids = Input(shape=(128,), dtype=tf.int32, name=\"segment_ids\")\n",
    "\n",
    "# Get BERT outputs\n",
    "sequence_output = bert_backbone([input_ids, input_mask, input_type_ids])\n",
    "pooled_output = sequence_output[:, 0, :]  # Use [CLS] token output\n",
    "\n",
    "# Add classification head\n",
    "x = Dropout(0.1)(pooled_output)\n",
    "outputs = Dense(num_classes, activation=\"softmax\")(x)\n",
    "\n",
    "# Create model\n",
    "model = Model(\n",
    "    inputs=[input_ids, input_mask, input_type_ids],\n",
    "    outputs=outputs\n",
    ")\n",
    "\n",
    "# Print model summary\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5: Compile and Train\n",
    "# Compile the model\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss=\"sparse_categorical_crossentropy\",\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n",
    "\n",
    "# Add callbacks\n",
    "callbacks = [\n",
    "    tf.keras.callbacks.EarlyStopping(\n",
    "        monitor='val_loss',\n",
    "        patience=3,\n",
    "        restore_best_weights=True\n",
    "    ),\n",
    "    tf.keras.callbacks.ModelCheckpoint(\n",
    "        'best_model.h5',\n",
    "        monitor='val_accuracy',\n",
    "        save_best_only=True\n",
    "    )\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    x_train,\n",
    "    y_train,\n",
    "    epochs=10,\n",
    "    batch_size=32,\n",
    "    validation_split=0.2,\n",
    "    callbacks=callbacks\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6: Evaluate and Visualize Results\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Evaluate the model\n",
    "test_results = model.evaluate(x_test, y_test)\n",
    "print(f\"Test accuracy: {test_results[1]:.4f}\")\n",
    "\n",
    "# Plot training history\n",
    "plt.figure(figsize=(12, 4))\n",
    "\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.title('Model Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(history.history['loss'], label='Training Loss')\n",
    "plt.plot(history.history['val_loss'], label='Validation Loss')\n",
    "plt.title('Model Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 7: Make Predictions\n",
    "def predict_email(text):\n",
    "    # Preprocess the input text\n",
    "    processed_text = preprocessor([text])\n",
    "    \n",
    "    # Make prediction\n",
    "    prediction = model.predict(processed_text)\n",
    "    predicted_class = label_encoder.inverse_transform([np.argmax(prediction[0])])[0]\n",
    "    confidence = np.max(prediction[0])\n",
    "    \n",
    "    return {\n",
    "        'predicted_class': predicted_class,\n",
    "        'confidence': confidence\n",
    "    }\n",
    "\n",
    "# Example usage\n",
    "sample_email = \"Your sample email text here\"\n",
    "result = predict_email(sample_email)\n",
    "print(f\"Predicted class: {result['predicted_class']}\")\n",
    "print(f\"Confidence: {result['confidence']:.2f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
